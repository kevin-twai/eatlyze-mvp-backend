*** Begin Patch
*** Update File: backend/app/services/openai_client.py
@@
-async def vision_analyze_base64(base64_str: str) -> str:
-    completion = await client.chat.completions.create(
-        model="gpt-4o-mini",
-        messages=[
-            {"role": "system", "content": VISION_PROMPT},
-            {"role": "user", "content": [
-                {"type": "text", "text": "請分析這張圖片的可食食材"},
-                {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64," + base64_str}}
-            ]}
-        ],
-        max_tokens=800,
-        temperature=0.2,
-    )
-    return completion.choices[0].message.content
+async def vision_analyze_base64(base64_str: str) -> str:
+    """呼叫 OpenAI，任何錯誤都丟 RuntimeError 回上層，由路由轉成 JSON。"""
+    from openai import OpenAIError
+    import httpx
+    try:
+        completion = await client.chat.completions.create(
+            model="gpt-4o-mini",
+            messages=[
+                {"role": "system", "content": VISION_PROMPT},
+                {"role": "user", "content": [
+                    {"type": "text", "text": "請分析這張圖片的可食食材"},
+                    {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64," + base64_str, "detail": "high"}}
+                ]}
+            ],
+            max_tokens=800,
+            temperature=0.2,
+        )
+        return completion.choices[0].message.content or ""
+    except (OpenAIError, httpx.HTTPError) as e:
+        raise RuntimeError(f"openai_call_failed: {type(e).__name__}: {e}") from e
*** End Patch
*** Begin Patch
*** Update File: backend/app/routers/analyze.py
@@
-from fastapi import APIRouter, UploadFile, File
+from fastapi import APIRouter, UploadFile, File
 from ..models import AnalyzeResponse, VisionResult, VisionItem
 from ..utils.normalizer import normalize_name
 from ..services.openai_client import vision_analyze_base64
-import json, base64, logging
+import json, base64, logging
+from typing import Optional
 
 router = APIRouter()
 logger = logging.getLogger(__name__)
 
-@router.post("/analyze/image", response_model=AnalyzeResponse)
-async def analyze_image(file: UploadFile = File(...)):
-    image_bytes = await file.read()
+@router.post("/analyze/image", response_model=AnalyzeResponse)
+async def analyze_image(
+    file: UploadFile | None = File(None),
+    image: UploadFile | None = File(None),
+):
+    up = file or image
+    if up is None:
+        return AnalyzeResponse(status="fail", reason="no_file_field")
+    ct = (up.content_type or "").lower()
+    if not (ct.startswith("image/") or ct in ("application/octet-stream", "")):
+        return AnalyzeResponse(status="fail", reason="invalid_file_type", debug={"content_type": ct})
+    image_bytes = await up.read()
+    if not image_bytes or len(image_bytes) < 128:
+        return AnalyzeResponse(status="fail", reason="empty_file")
+    if len(image_bytes) > 12 * 1024 * 1024:
+        return AnalyzeResponse(status="fail", reason="file_too_large", debug={"size_bytes": len(image_bytes)})
     image_b64 = base64.b64encode(image_bytes).decode("utf-8")
-    reply_text = await vision_analyze_base64(image_b64)
+    # 呼叫 OpenAI；任何錯誤轉為 fail JSON 而不是 500
+    try:
+        reply_text = await vision_analyze_base64(image_b64)
+    except Exception as e:
+        logger.exception("vision_call_failed")
+        return AnalyzeResponse(status="fail", reason="openai_call_failed", debug={"error": str(e)[:800]})
     try:
         raw = json.loads(reply_text)
         items = raw.get("items", [])
     except Exception:
         logger.exception("vision_json_parse_failed")
-        return AnalyzeResponse(status="fail", reason="vision_json_parse_failed", debug={"raw": reply_text[:800]})
+        return AnalyzeResponse(status="fail", reason="vision_json_parse_failed", debug={"raw": reply_text[:800]})
*** End Patch
